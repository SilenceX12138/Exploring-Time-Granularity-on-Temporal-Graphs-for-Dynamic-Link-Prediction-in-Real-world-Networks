{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "granularity_list = [1, 60, 3600, 3600 * 24, 3600 * 24 * 30, 3600 * 24 * 30 * 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_edges(data_df, split_timestamp_list):\n",
    "    \"\"\"compute the number of edges in different splits\n",
    "\n",
    "    Args:\n",
    "        data_df (pd.DataFrame): dataset\n",
    "        split_timestamp_list (list): timestamp for validation and test set\n",
    "\n",
    "    Returns:\n",
    "        list: the numbers of edges in different splits\n",
    "    \"\"\"\n",
    "    num_edge_list = []\n",
    "\n",
    "    train_split = data_df[data_df['ts'] <= split_timestamp_list[0]]['ts']\n",
    "    valid_split = data_df[(data_df['ts'] > split_timestamp_list[0])\n",
    "                          & (data_df['ts'] <= split_timestamp_list[1])]['ts']\n",
    "    test_split = data_df[data_df['ts'] > split_timestamp_list[1]]['ts']\n",
    "    num_edge_list.append(len(train_split))\n",
    "    num_edge_list.append(len(valid_split))\n",
    "    num_edge_list.append(len(test_split))\n",
    "\n",
    "    return num_edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_steps(data_df, split_timestamp_list):\n",
    "    \"\"\"compute the number of unique steps in different splits\n",
    "\n",
    "    Args:\n",
    "        data_df (pd.DataFrame): dataset\n",
    "        split_timestamp_list (list): timestamp for validation and test set\n",
    "\n",
    "    Returns:\n",
    "        list: the numbers of unique steps in different splits\n",
    "    \"\"\"\n",
    "    num_step_list = []\n",
    "\n",
    "    train_split = data_df[data_df['ts'] <= split_timestamp_list[0]]['ts']\n",
    "    valid_split = data_df[(data_df['ts'] > split_timestamp_list[0])\n",
    "                          & (data_df['ts'] <= split_timestamp_list[1])]['ts']\n",
    "    test_split = data_df[data_df['ts'] > split_timestamp_list[1]]['ts']\n",
    "    num_step_list.append(len(train_split.value_counts()))\n",
    "    num_step_list.append(len(valid_split.value_counts()))\n",
    "    num_step_list.append(len(test_split.value_counts()))\n",
    "\n",
    "    return num_step_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152757, 42569, 745, 32, 3, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = '../DG_data/TG_network_datasets/wikipedia/ml_wikipedia.csv'\n",
    "data_df = pd.read_csv(data_file)\n",
    "data_df = data_df.drop(axis=1, columns=['Unnamed: 0'])\n",
    "num_step_list = []\n",
    "for granularity in granularity_list:\n",
    "    num_step_list.append(len((np.ceil(data_df['ts'] / granularity)).value_counts()))\n",
    "num_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([102799, 27574, 27101], [99701, 26697, 26359])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = 60 * 60 * 24\n",
    "split_timestamp_list = [20 * day, 25 * day]\n",
    "get_split_edges(data_df, split_timestamp_list), get_split_steps(data_df, split_timestamp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[588918, 44640, 745, 32, 3, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = '../DG_data/TG_network_datasets/reddit/ml_reddit.csv'\n",
    "data_df = pd.read_csv(data_file)\n",
    "data_df = data_df.drop(axis=1, columns=['Unnamed: 0'])\n",
    "num_step_list = []\n",
    "for granularity in granularity_list:\n",
    "    num_step_list.append(len((np.ceil(data_df['ts'] / granularity)).value_counts()))\n",
    "num_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([434680, 110586, 127181], [432543, 110004, 126518])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = 60 * 60 * 24\n",
    "split_timestamp_list = [20 * day, 25 * day]\n",
    "get_split_edges(data_df, split_timestamp_list), get_split_steps(data_df, split_timestamp_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[345600, 33276, 707, 31, 2, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = '../DG_data/TG_network_datasets/mooc/ml_mooc.csv'\n",
    "data_df = pd.read_csv(data_file)\n",
    "data_df = data_df.drop(axis=1, columns=['Unnamed: 0'])\n",
    "num_step_list = []\n",
    "for granularity in granularity_list:\n",
    "    num_step_list.append(len((np.ceil(data_df['ts'] / granularity)).value_counts()))\n",
    "num_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([255776, 78732, 77241], [216364, 65815, 63421])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = 60 * 60 * 24\n",
    "split_timestamp_list = [20 * day, 25 * day]\n",
    "get_split_edges(data_df, split_timestamp_list), get_split_steps(data_df, split_timestamp_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LastFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1283614, 922509, 37542, 1587, 54, 6]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = '../DG_data/TG_network_datasets/lastfm/ml_lastfm.csv'\n",
    "data_df = pd.read_csv(data_file)\n",
    "data_df = data_df.drop(axis=1, columns=['Unnamed: 0'])\n",
    "num_step_list = []\n",
    "for granularity in granularity_list:\n",
    "    num_step_list.append(len((np.ceil(data_df['ts'] / granularity)).value_counts()))\n",
    "num_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([921756, 344470, 26877], [916312, 340736, 26566])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = 60 * 60 * 24\n",
    "split_timestamp_list = [1216 * day, 1520 * day]\n",
    "get_split_edges(data_df, split_timestamp_list), get_split_steps(data_df, split_timestamp_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22632, 22189, 8079, 1032, 45, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = '../DG_data/TG_network_datasets/enron/ml_enron.csv'\n",
    "data_df = pd.read_csv(data_file)\n",
    "data_df = data_df.drop(axis=1, columns=['Unnamed: 0'])\n",
    "num_step_list = []\n",
    "for granularity in granularity_list:\n",
    "    num_step_list.append(len((np.ceil(data_df['ts'] / granularity)).value_counts()))\n",
    "num_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([37440, 41866, 45929], [6224, 6357, 10051])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = 60 * 60 * 24\n",
    "split_timestamp_list = [730 * day, 912 * day]\n",
    "get_split_edges(data_df, split_timestamp_list), get_split_steps(data_df, split_timestamp_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social. Evo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[565932, 242886, 5685, 244, 10, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = '../DG_data/TG_network_datasets/SocialEvo/ml_SocialEvo.csv'\n",
    "data_df = pd.read_csv(data_file)\n",
    "data_df = data_df.drop(axis=1, columns=['Unnamed: 0'])\n",
    "num_step_list = []\n",
    "for granularity in granularity_list:\n",
    "    num_step_list.append(len((np.ceil(data_df['ts'] / granularity)).value_counts()))\n",
    "num_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([945394, 568867, 585258], [268758, 136849, 160325])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = 60 * 60 * 24\n",
    "split_timestamp_list = [160 * day, 200 * day]\n",
    "get_split_edges(data_df, split_timestamp_list), get_split_steps(data_df, split_timestamp_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58911, 35908, 3313, 192, 8, 2]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = '../DG_data/TG_network_datasets/uci/ml_uci.csv'\n",
    "data_df = pd.read_csv(data_file)\n",
    "data_df = data_df.drop(axis=1, columns=['Unnamed: 0'])\n",
    "num_step_list = []\n",
    "for granularity in granularity_list:\n",
    "    num_step_list.append(len((np.ceil(data_df['ts'] / granularity)).value_counts()))\n",
    "num_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([56080, 2404, 1351], [55202, 2402, 1307])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = 60 * 60 * 24\n",
    "split_timestamp_list = [130 * day, 162 * day]\n",
    "get_split_edges(data_df, split_timestamp_list), get_split_steps(data_df, split_timestamp_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "low-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
