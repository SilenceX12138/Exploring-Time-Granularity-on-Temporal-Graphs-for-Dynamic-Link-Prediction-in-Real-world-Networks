{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list     = ['reddit', 'enron', 'lastfm', 'mooc', 'uci', 'wikipedia', 'SocialEvo']\n",
    "granularity_list = [1, 60, 3600, 3600 * 24, 3600 * 24 * 30, 3600 * 24 * 30 * 12]\n",
    "granularity2text = {\n",
    "         1             : 'second',\n",
    "         60            : 'minute',\n",
    "         3600          : 'hour',\n",
    "    3600 * 24          : 'day',\n",
    "    3600 * 24 * 30     : 'month',\n",
    "    3600 * 24 * 30 * 12: 'year'\n",
    "}\n",
    "\n",
    "data_csv      = '../DG_data/TG_network_datasets/{}/ml_{}.csv'\n",
    "data_node_npy = '../DG_data/TG_network_datasets/{}/ml_{}_node.npy'\n",
    "data_npy      = '../DG_data/TG_network_datasets/{}/ml_{}.npy'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from ../DG_data/TG_network_datasets/reddit/ml_reddit.csv\n",
      "Data saved to ../DG_data_coarse/reddit/second/ml_reddit.csv\n",
      "Data saved to ../DG_data_coarse/reddit/minute/ml_reddit.csv\n",
      "Data saved to ../DG_data_coarse/reddit/hour/ml_reddit.csv\n",
      "Data saved to ../DG_data_coarse/reddit/day/ml_reddit.csv\n",
      "Data saved to ../DG_data_coarse/reddit/month/ml_reddit.csv\n",
      "Data saved to ../DG_data_coarse/reddit/year/ml_reddit.csv\n",
      "Data loaded from ../DG_data/TG_network_datasets/enron/ml_enron.csv\n",
      "Data saved to ../DG_data_coarse/enron/second/ml_enron.csv\n",
      "Data saved to ../DG_data_coarse/enron/minute/ml_enron.csv\n",
      "Data saved to ../DG_data_coarse/enron/hour/ml_enron.csv\n",
      "Data saved to ../DG_data_coarse/enron/day/ml_enron.csv\n",
      "Data saved to ../DG_data_coarse/enron/month/ml_enron.csv\n",
      "Data saved to ../DG_data_coarse/enron/year/ml_enron.csv\n",
      "Data loaded from ../DG_data/TG_network_datasets/lastfm/ml_lastfm.csv\n",
      "Data saved to ../DG_data_coarse/lastfm/second/ml_lastfm.csv\n",
      "Data saved to ../DG_data_coarse/lastfm/minute/ml_lastfm.csv\n",
      "Data saved to ../DG_data_coarse/lastfm/hour/ml_lastfm.csv\n",
      "Data saved to ../DG_data_coarse/lastfm/day/ml_lastfm.csv\n",
      "Data saved to ../DG_data_coarse/lastfm/month/ml_lastfm.csv\n",
      "Data saved to ../DG_data_coarse/lastfm/year/ml_lastfm.csv\n",
      "Data loaded from ../DG_data/TG_network_datasets/mooc/ml_mooc.csv\n",
      "Data saved to ../DG_data_coarse/mooc/second/ml_mooc.csv\n",
      "Data saved to ../DG_data_coarse/mooc/minute/ml_mooc.csv\n",
      "Data saved to ../DG_data_coarse/mooc/hour/ml_mooc.csv\n",
      "Data saved to ../DG_data_coarse/mooc/day/ml_mooc.csv\n",
      "Data saved to ../DG_data_coarse/mooc/month/ml_mooc.csv\n",
      "Data saved to ../DG_data_coarse/mooc/year/ml_mooc.csv\n",
      "Data loaded from ../DG_data/TG_network_datasets/uci/ml_uci.csv\n",
      "Data saved to ../DG_data_coarse/uci/second/ml_uci.csv\n",
      "Data saved to ../DG_data_coarse/uci/minute/ml_uci.csv\n",
      "Data saved to ../DG_data_coarse/uci/hour/ml_uci.csv\n",
      "Data saved to ../DG_data_coarse/uci/day/ml_uci.csv\n",
      "Data saved to ../DG_data_coarse/uci/month/ml_uci.csv\n",
      "Data saved to ../DG_data_coarse/uci/year/ml_uci.csv\n",
      "Data loaded from ../DG_data/TG_network_datasets/wikipedia/ml_wikipedia.csv\n",
      "Data saved to ../DG_data_coarse/wikipedia/second/ml_wikipedia.csv\n",
      "Data saved to ../DG_data_coarse/wikipedia/minute/ml_wikipedia.csv\n",
      "Data saved to ../DG_data_coarse/wikipedia/hour/ml_wikipedia.csv\n",
      "Data saved to ../DG_data_coarse/wikipedia/day/ml_wikipedia.csv\n",
      "Data saved to ../DG_data_coarse/wikipedia/month/ml_wikipedia.csv\n",
      "Data saved to ../DG_data_coarse/wikipedia/year/ml_wikipedia.csv\n",
      "Data loaded from ../DG_data/TG_network_datasets/SocialEvo/ml_SocialEvo.csv\n",
      "Data saved to ../DG_data_coarse/SocialEvo/second/ml_SocialEvo.csv\n",
      "Data saved to ../DG_data_coarse/SocialEvo/minute/ml_SocialEvo.csv\n",
      "Data saved to ../DG_data_coarse/SocialEvo/hour/ml_SocialEvo.csv\n",
      "Data saved to ../DG_data_coarse/SocialEvo/day/ml_SocialEvo.csv\n",
      "Data saved to ../DG_data_coarse/SocialEvo/month/ml_SocialEvo.csv\n",
      "Data saved to ../DG_data_coarse/SocialEvo/year/ml_SocialEvo.csv\n"
     ]
    }
   ],
   "source": [
    "for dataset in dataset_list:\n",
    "    dataset_path = '../DG_data_coarse/{}'.format(dataset)\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "    # load datasets\n",
    "    data_path = data_csv.format(dataset, dataset)\n",
    "    print('Data loaded from {}'.format(data_path))\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    data_df = data_df.drop(axis=1, columns=['Unnamed: 0'])        \n",
    "    \n",
    "    # create datasets of different granularities\n",
    "    for granularity in granularity_list:\n",
    "        granularity_path = os.path.join(dataset_path, granularity2text[granularity])\n",
    "        os.makedirs(granularity_path, exist_ok=True)\n",
    "        \n",
    "        temp_data_df = data_df.copy(deep=True)\n",
    "        temp_data_df['ts'] = (np.ceil(temp_data_df['ts'] / granularity) * granularity)\n",
    "        \n",
    "        new_data_path = os.path.join(granularity_path, 'ml_{}.csv'.format(dataset))\n",
    "        temp_data_df.to_csv(new_data_path)\n",
    "        print('Data saved to {}'.format(new_data_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(data_df, val_ratio=0.15, test_ratio=0.15):\n",
    "    val_time, test_time = list(np.quantile(data_df['ts'], [(1 - val_ratio - test_ratio), (1 - test_ratio)]))\n",
    "    num_train, num_val, num_test = len(data_df[data_df['ts'] <= val_time]), len(data_df[(data_df['ts'] > val_time)&(data_df['ts'] <= test_time)]), len(data_df[data_df['ts'] > test_time])\n",
    "    return num_train, num_val, num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit    |second  |  470713|  100867|  100867\n",
      "reddit    |minute  |  470722|  100858|  100867\n",
      "reddit    |hour    |  470815|  101296|  100336\n",
      "reddit    |day     |  475299|  114325|   82823\n",
      "reddit    |month   |  650442|       0|   22005\n",
      "reddit    |year    |  672447|       0|       0\n",
      "enron     |second  |   87664|   18786|   18785\n",
      "enron     |minute  |   87664|   18786|   18785\n",
      "enron     |hour    |   87665|   18808|   18762\n",
      "enron     |day     |   87743|   18971|   18521\n",
      "enron     |month   |   89523|   24162|   11550\n",
      "enron     |year    |  105631|   19604|       0\n",
      "lastfm    |second  |  905172|  193965|  193966\n",
      "lastfm    |minute  |  905174|  193963|  193966\n",
      "lastfm    |hour    |  905232|  193919|  193952\n",
      "lastfm    |day     |  905232|  194399|  193472\n",
      "lastfm    |month   |  936951|  182053|  174099\n",
      "lastfm    |year    | 1159991|       0|  133112\n",
      "mooc      |second  |  288224|   61762|   61763\n",
      "mooc      |minute  |  288224|   61770|   61755\n",
      "mooc      |hour    |  288480|   62271|   60998\n",
      "mooc      |day     |  301509|   62868|   47372\n",
      "mooc      |month   |  411749|       0|       0\n",
      "mooc      |year    |  411749|       0|       0\n",
      "uci       |second  |   41884|    8975|    8976\n",
      "uci       |minute  |   41885|    8974|    8976\n",
      "uci       |hour    |   41895|    8964|    8976\n",
      "uci       |day     |   42079|    8790|    8966\n",
      "uci       |month   |   49409|    3323|    7103\n",
      "uci       |year    |   59835|       0|       0\n",
      "wikipedia |second  |  110232|   23621|   23621\n",
      "wikipedia |minute  |  110237|   23620|   23617\n",
      "wikipedia |hour    |  110368|   23754|   23352\n",
      "wikipedia |day     |  112937|   22904|   21633\n",
      "wikipedia |month   |  153823|       0|    3651\n",
      "wikipedia |year    |  157474|       0|       0\n",
      "SocialEvo |second  | 1469665|  314930|  314924\n",
      "SocialEvo |minute  | 1469671|  314924|  314924\n",
      "SocialEvo |hour    | 1470682|  314396|  314441\n",
      "SocialEvo |day     | 1474123|  317858|  307538\n",
      "SocialEvo |month   | 1668041|  424508|    6970\n",
      "SocialEvo |year    | 2099519|       0|       0\n"
     ]
    }
   ],
   "source": [
    "for dataset in dataset_list:\n",
    "    dataset_path = '../DG_data_coarse/{}'.format(dataset)\n",
    "    \n",
    "    for granularity in granularity_list:\n",
    "        granularity_path = os.path.join(dataset_path, granularity2text[granularity])\n",
    "        \n",
    "        data_csv = os.path.join(granularity_path, 'ml_{}.csv'.format(dataset))\n",
    "        data_df = pd.read_csv(data_csv)\n",
    "        \n",
    "        num_train, num_val, num_test = get_split(data_df)\n",
    "        print('{:10}|{:8}|{:8}|{:8}|{:8}'.format(dataset, granularity2text[granularity], num_train, num_val, num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1469665|  314930|  314924\n",
    "SocialEvo |minute  | 1469671|  314924|  314924\n",
    "SocialEvo |hour    | 1470682|  314396|  314441\n",
    "SocialEvo |day     | 1474123|  317858|  307538\n",
    "SocialEvo |month   | 1668041|  424508|    6970\n",
    "SocialEvo |year    | 2099519"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "low-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
